{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exist,no need to download\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "data_path = Path(\"data\")\n",
    "image_path = data_path / \"pizza_steak_sushi\"\n",
    "\n",
    "if image_path.is_dir() :\n",
    "    print(\"Folder already exist,no need to download\")\n",
    "else:\n",
    "    data_path.mkdir(parents=True,exist_ok=True)\n",
    "    with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
    "        print(\"Downloading data...\")\n",
    "        res = requests.get(\n",
    "            \"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\"\n",
    "        )\n",
    "        f.write(res.content)\n",
    "\n",
    "    with ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zf:\n",
    "        print(\"Extract file...\")\n",
    "        zf.extractall(image_path)\n",
    "\n",
    "    os.remove(data_path / \"pizza_steak_sushi.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on data\\pizza_steak_sushi\\train has 3 directories and 0 files\n",
      "on data\\pizza_steak_sushi\\train\\pizza has 0 directories and 154 files\n",
      "on data\\pizza_steak_sushi\\train\\steak has 0 directories and 146 files\n",
      "on data\\pizza_steak_sushi\\train\\sushi has 0 directories and 150 files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for dirpath,dirnames,filenames in os.walk(train_dir):\n",
    "    print(f\"on {dirpath} has {len(dirnames)} directories and {len(filenames)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manual Transforms\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "\n",
    "manual_transforms = v2.Compose(\n",
    "\n",
    "    [\n",
    "\n",
    "        v2.ToImage(),\n",
    "\n",
    "        v2.Resize(size=(256, 256), interpolation=InterpolationMode.BICUBIC),\n",
    "\n",
    "        v2.CenterCrop(size=(224, 224)),\n",
    "\n",
    "        v2.ToDtype(dtype=torch.float32, scale=True),\n",
    "\n",
    "        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet_B0_Weights.IMAGENET1K_V1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Auto Transforms\n",
    "from torchvision.models.efficientnet import EfficientNet_B0_Weights\n",
    "\n",
    "weights  = EfficientNet_B0_Weights.DEFAULT\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassification(\n",
       "    crop_size=[224]\n",
       "    resize_size=[256]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BICUBIC\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_transforms = weights.transforms()\n",
    "auto_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from going_modular.data_setup import create_dataloaders\n",
    "\n",
    "train_dataloader, test_dataloader, class_names = create_dataloaders(\n",
    "    train_dir, test_dir, manual_transforms, auto_transforms, BATCH_SIZE\n",
    ")\n",
    "\n",
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.efficientnet import efficientnet_b0,EfficientNet_B0_Weights\n",
    "\n",
    "weights = EfficientNet_B0_Weights.DEFAULT\n",
    "model = efficientnet_b0(weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "EfficientNet                                            [32, 1000]                --\n",
       "├─Sequential: 1-1                                       [32, 1280, 8, 8]          --\n",
       "│    └─Conv2dNormActivation: 2-1                        [32, 32, 128, 128]        --\n",
       "│    │    └─Conv2d: 3-1                                 [32, 32, 128, 128]        864\n",
       "│    │    └─BatchNorm2d: 3-2                            [32, 32, 128, 128]        64\n",
       "│    │    └─SiLU: 3-3                                   [32, 32, 128, 128]        --\n",
       "│    └─Sequential: 2-2                                  [32, 16, 128, 128]        --\n",
       "│    │    └─MBConv: 3-4                                 [32, 16, 128, 128]        1,448\n",
       "│    └─Sequential: 2-3                                  [32, 24, 64, 64]          --\n",
       "│    │    └─MBConv: 3-5                                 [32, 24, 64, 64]          6,004\n",
       "│    │    └─MBConv: 3-6                                 [32, 24, 64, 64]          10,710\n",
       "│    └─Sequential: 2-4                                  [32, 40, 32, 32]          --\n",
       "│    │    └─MBConv: 3-7                                 [32, 40, 32, 32]          15,350\n",
       "│    │    └─MBConv: 3-8                                 [32, 40, 32, 32]          31,290\n",
       "│    └─Sequential: 2-5                                  [32, 80, 16, 16]          --\n",
       "│    │    └─MBConv: 3-9                                 [32, 80, 16, 16]          37,130\n",
       "│    │    └─MBConv: 3-10                                [32, 80, 16, 16]          102,900\n",
       "│    │    └─MBConv: 3-11                                [32, 80, 16, 16]          102,900\n",
       "│    └─Sequential: 2-6                                  [32, 112, 16, 16]         --\n",
       "│    │    └─MBConv: 3-12                                [32, 112, 16, 16]         126,004\n",
       "│    │    └─MBConv: 3-13                                [32, 112, 16, 16]         208,572\n",
       "│    │    └─MBConv: 3-14                                [32, 112, 16, 16]         208,572\n",
       "│    └─Sequential: 2-7                                  [32, 192, 8, 8]           --\n",
       "│    │    └─MBConv: 3-15                                [32, 192, 8, 8]           262,492\n",
       "│    │    └─MBConv: 3-16                                [32, 192, 8, 8]           587,952\n",
       "│    │    └─MBConv: 3-17                                [32, 192, 8, 8]           587,952\n",
       "│    │    └─MBConv: 3-18                                [32, 192, 8, 8]           587,952\n",
       "│    └─Sequential: 2-8                                  [32, 320, 8, 8]           --\n",
       "│    │    └─MBConv: 3-19                                [32, 320, 8, 8]           717,232\n",
       "│    └─Conv2dNormActivation: 2-9                        [32, 1280, 8, 8]          --\n",
       "│    │    └─Conv2d: 3-20                                [32, 1280, 8, 8]          409,600\n",
       "│    │    └─BatchNorm2d: 3-21                           [32, 1280, 8, 8]          2,560\n",
       "│    │    └─SiLU: 3-22                                  [32, 1280, 8, 8]          --\n",
       "├─AdaptiveAvgPool2d: 1-2                                [32, 1280, 1, 1]          --\n",
       "├─Sequential: 1-3                                       [32, 1000]                --\n",
       "│    └─Dropout: 2-10                                    [32, 1280]                --\n",
       "│    └─Linear: 2-11                                     [32, 1000]                1,281,000\n",
       "=========================================================================================================\n",
       "Total params: 5,288,548\n",
       "Trainable params: 5,288,548\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 16.11\n",
       "=========================================================================================================\n",
       "Input size (MB): 25.17\n",
       "Forward/backward pass size (MB): 4508.38\n",
       "Params size (MB): 21.15\n",
       "Estimated Total Size (MB): 4554.70\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model=model,input_size=(BATCH_SIZE,3,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.2,inplace=True),\n",
    "    nn.Linear(in_features=1280,out_features=len(class_names),bias=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dropout(p=0.2, inplace=True)\n",
       "  (1): Linear(in_features=1280, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
